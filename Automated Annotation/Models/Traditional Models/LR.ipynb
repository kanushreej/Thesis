{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stance: pro_brexit\n",
      "Train Accuracy: 0.9993802675718972 | Validation Accuracy: 0.9744444444444443\n",
      "Train Precision: 0.9977971835418142 | Validation Precision: 0.5691503267973855\n",
      "Train Recall: 1.0 | Validation Recall: 0.8\n",
      "Train F1 Score: 0.998890325146664 | Validation F1 Score: 0.6413636363636364\n",
      "Warning: Potential overfitting detected for stance pro_brexit.\n",
      "Stance: anti_brexit\n",
      "Train Accuracy: 1.0 | Validation Accuracy: 0.93\n",
      "Train Precision: 1.0 | Validation Precision: 0.5915427181514138\n",
      "Train Recall: 1.0 | Validation Recall: 0.7279605263157894\n",
      "Train F1 Score: 1.0 | Validation F1 Score: 0.6464213378975895\n",
      "Warning: Potential overfitting detected for stance anti_brexit.\n",
      "Stance: neutral\n",
      "Train Accuracy: 1.0 | Validation Accuracy: 0.918888888888889\n",
      "Train Precision: 1.0 | Validation Precision: 0.8127775588163958\n",
      "Train Recall: 1.0 | Validation Recall: 0.9565217391304348\n",
      "Train F1 Score: 1.0 | Validation F1 Score: 0.8705572259843184\n",
      "Stance: irrelevant\n",
      "Train Accuracy: 0.9972717013968962 | Validation Accuracy: 0.9066666666666666\n",
      "Train Precision: 0.9892862531409895 | Validation Precision: 0.44691056910569105\n",
      "Train Recall: 0.9995535714285715 | Validation Recall: 0.3549079594722301\n",
      "Train F1 Score: 0.9943585007813613 | Validation F1 Score: 0.39442570746927696\n",
      "Warning: Potential overfitting detected for stance irrelevant.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def classify_issue(issue):\n",
    "    stance_groups = {\n",
    "        'brexit': ['pro_brexit', 'anti_brexit'],\n",
    "        'climateAction': ['pro_climateAction', 'anti_climateAction'],\n",
    "        'NHS': ['pro_NHS', 'anti_NHS'],\n",
    "        'israel_palestine': ['pro_israel', 'pro_palestine'],\n",
    "        'taxation': ['pro_company_taxation', 'pro_worker_taxation']\n",
    "    }\n",
    "    \n",
    "    if issue not in stance_groups:\n",
    "        raise ValueError(f\"Unknown issue: {issue}\")\n",
    "    \n",
    "    targets = stance_groups[issue] + ['neutral', 'irrelevant']\n",
    "\n",
    "    file_path = '/Users/adamzulficar/Documents/year3/Bachelor Project/Thesis/Automated Annotation/Training Data/UK/{}_training.csv'.format(issue)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    ## SMOTE ##\n",
    "\n",
    "    def str_to_array(s):\n",
    "        return np.fromstring(s.strip(\"[]\"), sep=' ')\n",
    "\n",
    "    features = df['text_vector'].apply(str_to_array).tolist()\n",
    "    context = df['context_vector'].apply(str_to_array).tolist()\n",
    "    X = np.array([np.concatenate((f, c)) for f, c in zip(features, context)])\n",
    "    y_combined = np.array(df[targets])\n",
    "\n",
    "    smote = SMOTE()\n",
    "    X_resampled, y_resampled_combined = smote.fit_resample(X, y_combined)\n",
    "\n",
    "    text_vector_length = len(str_to_array(df['text_vector'].iloc[0]))\n",
    "    context_vector_length = len(str_to_array(df['context_vector'].iloc[0]))\n",
    "\n",
    "    text_vectors_resampled = X_resampled[:, :text_vector_length]\n",
    "    context_vectors_resampled = X_resampled[:, text_vector_length:]\n",
    "\n",
    "    def array_to_str(arr):\n",
    "        return ' '.join(map(str, arr))\n",
    "\n",
    "    resampled_data = pd.DataFrame()\n",
    "    resampled_data['text_vector'] = list(map(array_to_str, text_vectors_resampled))\n",
    "    resampled_data['context_vector'] = list(map(array_to_str, context_vectors_resampled))\n",
    "\n",
    "    for i, target in enumerate(targets):\n",
    "        resampled_data[target] = y_resampled_combined[:, i]\n",
    "\n",
    "    data = resampled_data\n",
    "\n",
    "    ## FEATURE VECTOR PROCESSING ##\n",
    "\n",
    "    TEXT_VECTOR_SIZE = 100\n",
    "    CONTEXT_VECTOR_SIZE = 100\n",
    "\n",
    "    def extract_vectors(row):\n",
    "        text_vector = np.array(row['text_vector'].split(), dtype=float)\n",
    "        context_vector = np.array(row['context_vector'].split(), dtype=float)\n",
    "\n",
    "        if len(text_vector) > TEXT_VECTOR_SIZE:\n",
    "            text_vector = text_vector[:TEXT_VECTOR_SIZE]\n",
    "        else:\n",
    "            text_vector = np.pad(text_vector, (0, TEXT_VECTOR_SIZE - len(text_vector)), 'constant')\n",
    "\n",
    "        if len(context_vector) > CONTEXT_VECTOR_SIZE:\n",
    "            context_vector = context_vector[:CONTEXT_VECTOR_SIZE]\n",
    "        else:\n",
    "            context_vector = np.pad(context_vector, (0, CONTEXT_VECTOR_SIZE - len(context_vector)), 'constant')\n",
    "\n",
    "        return np.concatenate([text_vector, context_vector])\n",
    "    \n",
    "    ## MODEL ##\n",
    "\n",
    "    X = data.apply(extract_vectors, axis=1)\n",
    "    X = np.stack(X.values)\n",
    "    y = data[targets]\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    def resolve_contradictions(probabilities, stances):\n",
    "        resolved_stances = np.zeros_like(probabilities)\n",
    "        stance_groups = {\n",
    "            'brexit': ['pro_brexit', 'anti_brexit'],\n",
    "            'climateAction': ['pro_climateAction', 'anti_climateAction'],\n",
    "            'NHS': ['pro_NHS', 'anti_NHS'],\n",
    "            'israel_palestine': ['pro_israel', 'pro_palestine'],\n",
    "            'taxation': ['pro_company_taxation', 'pro_worker_taxation']\n",
    "        }\n",
    "\n",
    "        for i, prob in enumerate(probabilities):\n",
    "            prob_dict = {stance: prob[j] for j, stance in enumerate(stances)}\n",
    "\n",
    "            max_stance = max(prob_dict, key=prob_dict.get)\n",
    "            if max_stance in ['irrelevant', 'neutral']:\n",
    "                resolved_stances[i][stances.index(max_stance)] = 1\n",
    "            else:\n",
    "                any_above_threshold = any(p > 0.5 for p in prob_dict.values())\n",
    "                if any_above_threshold:\n",
    "                    for group in stance_groups[issue]:\n",
    "                        relevant_group = [s for s in group if s in stances]\n",
    "                        if relevant_group:\n",
    "                            max_stance = max(relevant_group, key=lambda x: prob_dict[x])\n",
    "                            if prob_dict[max_stance] > 0.5:  # Threshold can still be tuned\n",
    "                                resolved_stances[i][stances.index(max_stance)] = 1\n",
    "                else:\n",
    "                    max_stance = max(prob_dict, key=prob_dict.get)\n",
    "                    resolved_stances[i][stances.index(max_stance)] = 1\n",
    "\n",
    "        return resolved_stances\n",
    "\n",
    "    def evaluate_model(X, y, stances, n_splits=10):\n",
    "        results = {stance: {'train_accuracy': [], 'val_accuracy': [], 'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [], 'train_f1_score': [], 'val_f1_score': []} for stance in stances}\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=n_splits)\n",
    "\n",
    "        for train_index, test_index in skf.split(X, y['neutral']):  # Using 'neutral' just for stratification\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            for stance in stances:\n",
    "                param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'penalty': ['l2']}\n",
    "                grid_search = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced'), param_grid, cv=5, scoring='f1')\n",
    "                grid_search.fit(X_train, y_train[stance])\n",
    "\n",
    "                best_clf = grid_search.best_estimator_\n",
    "\n",
    "                y_train_pred = best_clf.predict(X_train)\n",
    "                y_test_pred = best_clf.predict(X_test)\n",
    "\n",
    "                results[stance]['train_accuracy'].append(accuracy_score(y_train[stance], y_train_pred))\n",
    "                results[stance]['val_accuracy'].append(accuracy_score(y_test[stance], y_test_pred))\n",
    "                results[stance]['train_precision'].append(precision_score(y_train[stance], y_train_pred, zero_division=0))\n",
    "                results[stance]['val_precision'].append(precision_score(y_test[stance], y_test_pred, zero_division=0))\n",
    "                results[stance]['train_recall'].append(recall_score(y_train[stance], y_train_pred, zero_division=0))\n",
    "                results[stance]['val_recall'].append(recall_score(y_test[stance], y_test_pred, zero_division=0))\n",
    "                results[stance]['train_f1_score'].append(f1_score(y_train[stance], y_train_pred, zero_division=0))\n",
    "                results[stance]['val_f1_score'].append(f1_score(y_test[stance], y_test_pred, zero_division=0))\n",
    "\n",
    "        for stance, metrics in results.items():\n",
    "            train_accuracy = np.mean(metrics['train_accuracy'])\n",
    "            val_accuracy = np.mean(metrics['val_accuracy'])\n",
    "            train_precision = np.mean(metrics['train_precision'])\n",
    "            val_precision = np.mean(metrics['val_precision'])\n",
    "            train_recall = np.mean(metrics['train_recall'])\n",
    "            val_recall = np.mean(metrics['val_recall'])\n",
    "            train_f1 = np.mean(metrics['train_f1_score'])\n",
    "            val_f1 = np.mean(metrics['val_f1_score'])\n",
    "\n",
    "            print(f\"Stance: {stance}\")\n",
    "            print(f\"Train Accuracy: {train_accuracy} | Validation Accuracy: {val_accuracy}\")\n",
    "            print(f\"Train Precision: {train_precision} | Validation Precision: {val_precision}\")\n",
    "            print(f\"Train Recall: {train_recall} | Validation Recall: {val_recall}\")\n",
    "            print(f\"Train F1 Score: {train_f1} | Validation F1 Score: {val_f1}\")\n",
    "\n",
    "            if val_f1 < train_f1 * 0.8:  # Example threshold for detecting overfitting\n",
    "                print(f\"Warning: Potential overfitting detected for stance {stance}.\")\n",
    "\n",
    "    def predict_and_resolve(X, stances):\n",
    "        predictions = []\n",
    "\n",
    "        for stance in stances:\n",
    "            clf = LogisticRegression(max_iter=1000, class_weight='balanced', C=0.1, penalty='l2')\n",
    "            clf.fit(X, y[stance])\n",
    "            stance_predictions = clf.predict_proba(X)[:, 1]\n",
    "            predictions.append(stance_predictions)\n",
    "\n",
    "        predictions = np.array(predictions).transpose(1, 0)\n",
    "        resolved_predictions = resolve_contradictions(predictions, stances)\n",
    "        \n",
    "        return resolved_predictions\n",
    "\n",
    "    evaluate_model(X, y, targets)\n",
    "\n",
    "classify_issue('brexit')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
