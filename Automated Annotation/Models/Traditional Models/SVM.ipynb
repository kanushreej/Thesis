{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Multilabel Classification with SVM and Overfitting Check"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "import pandas as pd\n",
       "import numpy as np\n",
       "from imblearn.over_sampling import SMOTE\n",
       "from sklearn.svm import SVC\n",
       "from sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve\n",
       "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
       "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n",
       "from sklearn.multiclass import OneVsRestClassifier\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "sns.set(style=\"whitegrid\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Data Preprocessing"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def classify_issue(issue):\n",
       "    stance_groups = {\n",
       "        'brexit': ['pro_brexit', 'anti_brexit'],\n",
       "        'climateAction': ['pro_climateAction', 'anti_climateAction'],\n",
       "        'NHS': ['pro_NHS', 'anti_NHS'],\n",
       "        'israel_palestine': ['pro_israel', 'pro_palestine'],\n",
       "        'taxation': ['pro_company_taxation', 'pro_worker_taxation']\n",
       "    }\n",
       "    \n",
       "    if issue not in stance_groups:\n",
       "        raise ValueError(f\"Unknown issue: {issue}\")\n",
       "    \n",
       "    targets = stance_groups[issue] + ['neutral', 'irrelevant']\n",
       "\n",
       "    file_path = '/Users/adamzulficar/Documents/year3/Bachelor Project/Thesis/Automated Annotation/Training Data/UK/{}_training.csv'.format(issue)\n",
       "    df = pd.read_csv(file_path)\n",
       "\n",
       "    ## SMOTE ##\n",
       "\n",
       "    def str_to_array(s):\n",
       "        return np.fromstring(s.strip(\"[]\"), sep=' ')\n",
       "\n",
       "    features = df['text_vector'].apply(str_to_array).tolist()\n",
       "    context = df['context_vector'].apply(str_to_array).tolist()\n",
       "    X = np.array([np.concatenate((f, c)) for f, c in zip(features, context)])\n",
       "    y_combined = df[targets].apply(lambda row: row[row == 1].index.tolist(), axis=1).tolist()\n",
       "\n",
       "    mlb = MultiLabelBinarizer(classes=targets)\n",
       "    y = mlb.fit_transform(y_combined)\n",
       "\n",
       "    smote = SMOTE()\n",
       "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
       "\n",
       "    text_vector_length = len(str_to_array(df['text_vector'].iloc[0]))\n",
       "    context_vector_length = len(str_to_array(df['context_vector'].iloc[0]))\n",
       "\n",
       "    text_vectors_resampled = X_resampled[:, :text_vector_length]\n",
       "    context_vectors_resampled = X_resampled[:, text_vector_length:]\n",
       "\n",
       "    def array_to_str(arr):\n",
       "        return ' '.join(map(str, arr))\n",
       "\n",
       "    resampled_data = pd.DataFrame()\n",
       "    resampled_data['text_vector'] = list(map(array_to_str, text_vectors_resampled))\n",
       "    resampled_data['context_vector'] = list(map(array_to_str, context_vectors_resampled))\n",
       "\n",
       "    for i, target in enumerate(targets):\n",
       "        resampled_data[target] = y_resampled[:, i]\n",
       "\n",
       "    data = resampled_data\n",
       "\n",
       "    ## FEATURE VECTOR PROCESSING ##\n",
       "\n",
       "    TEXT_VECTOR_SIZE = 100\n",
       "    CONTEXT_VECTOR_SIZE = 100\n",
       "\n",
       "    def extract_vectors(row):\n",
       "        text_vector = np.array(row['text_vector'].split(), dtype=float)\n",
       "        context_vector = np.array(row['context_vector'].split(), dtype=float)\n",
       "\n",
       "        if len(text_vector) > TEXT_VECTOR_SIZE:\n",
       "            text_vector = text_vector[:TEXT_VECTOR_SIZE]\n",
       "        else:\n",
       "            text_vector = np.pad(text_vector, (0, TEXT_VECTOR_SIZE - len(text_vector)), 'constant')\n",
       "\n",
       "        if len(context_vector) > CONTEXT_VECTOR_SIZE:\n",
       "            context_vector = context_vector[:CONTEXT_VECTOR_SIZE]\n",
       "        else:\n",
       "            context_vector = np.pad(context_vector, (0, CONTEXT_VECTOR_SIZE - len(context_vector)), 'constant')\n",
       "\n",
       "        return np.concatenate([text_vector, context_vector])\n",
       "    \n",
       "    ## MODEL ##\n",
       "\n",
       "    X = data.apply(extract_vectors, axis=1)\n",
       "    X = np.stack(X.values)\n",
       "    y = data[targets].values\n",
       "\n",
       "    # Scale the features\n",
       "    scaler = StandardScaler()\n",
       "    X = scaler.fit_transform(X)\n",
       "\n",
       "    return X, y, targets\n",
       "\n",
       "X, y, targets = classify_issue('brexit')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Model Training and Cross-Validation"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def evaluate_model(X, y, stances, n_splits=10, random_state=42):\n",
       "    results = {stance: {'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'predictions': [], 'test_labels': []} for stance in stances}\n",
       "    overall_predictions = []\n",
       "    overall_true_labels = []\n",
       "\n",
       "    skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
       "\n",
       "    param_grid = {\n",
       "        'estimator__C': [1, 10, 100, 1000],\n",
       "        'estimator__gamma': [1, 0.1, 0.01, 0.001],\n",
       "        'estimator__kernel': ['rbf', 'linear']\n",
       "    }\n",
       "\n",
       "    for train_index, test_index in skf.split(X, y[:, stances.index('neutral')]):  # Using 'neutral' just for stratification\n",
       "        X_train, X_test = X[train_index], X[test_index]\n",
       "        y_train, y_test = y[train_index], y[test_index]\n",
       "\n",
       "        clf = OneVsRestClassifier(SVC(probability=True, class_weight='balanced'))\n",
       "        grid_search = GridSearchCV(clf, param_grid, scoring='f1_weighted', cv=3)\n",
       "        grid_search.fit(X_train, y_train)\n",
       "\n",
       "        best_clf = grid_search.best_estimator_\n",
       "\n",
       "        y_pred_prob = best_clf.predict_proba(X_test)\n",
       "\n",
       "        # Select only the label with the highest probability for each instance\n",
       "        y_pred = np.zeros_like(y_pred_prob)\n",
       "        max_prob_indices = np.argmax(y_pred_prob, axis=1)\n",
       "        for i, idx in enumerate(max_prob_indices):\n",
       "            y_pred[i, idx] = 1\n",
       "\n",
       "        for i, stance in enumerate(stances):\n",
       "            y_pred_stance = y_pred[:, i]\n",
       "            y_true_stance = y_test[:, i]\n",
       "\n",
       "            results[stance]['accuracy'].append(accuracy_score(y_true_stance, y_pred_stance))\n",
       "            results[stance]['precision'].append(precision_score(y_true_stance, y_pred_stance, zero_division=0))\n",
       "            results[stance]['recall'].append(recall_score(y_true_stance, y_pred_stance, zero_division=0))\n",
       "            results[stance]['f1_score'].append(f1_score(y_true_stance, y_pred_stance, zero_division=0))\n",
       "            results[stance]['predictions'].extend(y_pred_stance)\n",
       "            results[stance]['test_labels'].extend(y_true_stance)\n",
       "\n",
       "            overall_predictions.extend(y_pred_stance)\n",
       "            overall_true_labels.extend(y_true_stance)\n",
       "\n",
       "    overall_metrics = {'Stance': [], 'Accuracy': [], 'Precision': [], 'Recall': [], 'F1 Score': []}\n",
       "    \n",
       "    for stance, metrics in results.items():\n",
       "        accuracy = np.mean(metrics['accuracy'])\n",
       "        precision = np.mean(metrics['precision'])\n",
       "        recall = np.mean(metrics['recall'])\n",
       "        f1 = np.mean(metrics['f1_score'])\n",
       "\n",
       "        overall_metrics['Stance'].append(stance)\n",
       "        overall_metrics['Accuracy'].append(accuracy)\n",
       "        overall_metrics['Precision'].append(precision)\n",
       "        overall_metrics['Recall'].append(recall)\n",
       "        overall_metrics['F1 Score'].append(f1)\n",
       "        \n",
       "        print(f\"Stance: {stance}\")\n",
       "        print(f\"Accuracy: {accuracy}\")\n",
       "        print(f\"Precision: {precision}\")\n",
       "        print(f\"Recall: {recall}\")\n",
       "        print(f\"F1 Score: {f1}\")\n",
       "\n",
       "        predicted_counts = np.bincount(metrics['predictions'])\n",
       "        actual_counts = np.bincount(metrics['test_labels'])\n",
       "        print(f\"Predicted counts: {predicted_counts}\")\n",
       "        print(f\"Actual counts: {actual_counts}\")\n",
       "        print(\"\\n\")\n",
       "\n",
       "    overall_accuracy = accuracy_score(overall_true_labels, overall_predictions)\n",
       "    overall_precision = precision_score(overall_true_labels, overall_predictions, zero_division=0)\n",
       "    overall_recall = recall_score(overall_true_labels, overall_predictions, zero_division=0)\n",
       "    overall_f1 = f1_score(overall_true_labels, overall_predictions, zero_division=0)\n",
       "\n",
       "    print(f\"Overall Accuracy: {overall_accuracy}\")\n",
       "    print(f\"Overall Precision: {overall_precision}\")\n",
       "    print(f\"Overall Recall: {overall_recall}\")\n",
       "    print(f\"Overall F1 Score: {overall_f1}\")\n",
       "\n",
       "    overall_metrics_df = pd.DataFrame(overall_metrics)\n",
       "    print(\"Overall Performance Metrics by Stance:\\n\", overall_metrics_df)\n",
       "\n",
       "    return best_clf, overall_metrics_df\n",
       "\n",
       "best_clf, overall_metrics_df = evaluate_model(X, y, targets)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Overfitting Check"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "def plot_learning_curve(estimator, X, y, cv, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
       "    plt.figure(figsize=(12, 8))\n",
       "    plt.title(\"Learning Curve\")\n",
       "    plt.xlabel(\"Training examples\")\n",
       "    plt.ylabel(\"Score\")\n",
       "\n",
       "    train_sizes, train_scores, test_scores = learning_curve(\n",
       "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='f1_weighted')\n",
       "\n",
       "    train_scores_mean = np.mean(train_scores, axis=1)\n",
       "    train_scores_std = np.std(train_scores, axis=1)\n",
       "    test_scores_mean = np.mean(test_scores, axis=1)\n",
       "    test_scores_std = np.std(test_scores, axis=1)\n",
       "\n",
       "    plt.grid()\n",
       "\n",
       "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
       "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
       "                     color=\"r\")\n",
       "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
       "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
       "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
       "             label=\"Training score\")\n",
       "\n",
       "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
       "             label=\"Cross-validation score\")\n",
       "\n",
       "    plt.legend(loc=\"best\")\n",
       "    plt.show()\n",
       "\n",
       "cv = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
       "plot_learning_curve(best_clf, X, y, cv)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Example Learning Curves"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Example of Overfitting"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# Example plot for overfitting (simulated)\n",
       "train_sizes = [50, 100, 150, 200, 250]\n",
       "train_scores = [0.95, 0.97, 0.98, 0.99, 1.0]\n",
       "test_scores = [0.6, 0.65, 0.63, 0.62, 0.61]\n",
       "\n",
       "plt.figure(figsize=(12, 8))\n",
       "plt.title(\"Example of Overfitting\")\n",
       "plt.xlabel(\"Training examples\")\n",
       "plt.ylabel(\"Score\")\n",
       "\n",
       "plt.plot(train_sizes, train_scores, 'o-', color=\"r\", label=\"Training score\")\n",
       "plt.plot(train_sizes, test_scores, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
       "\n",
       "plt.legend(loc=\"best\")\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Example of Underfitting"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# Example plot for underfitting (simulated)\n",
       "train_sizes = [50, 100, 150, 200, 250]\n",
       "train_scores = [0.6, 0.62, 0.63, 0.64, 0.65]\n",
       "test_scores = [0.55, 0.56, 0.57, 0.56, 0.55]\n",
       "\n",
       "plt.figure(figsize=(12, 8))\n",
       "plt.title(\"Example of Underfitting\")\n",
       "plt.xlabel(\"Training examples\")\n",
       "plt.ylabel(\"Score\")\n",
       "\n",
       "plt.plot(train_sizes, train_scores, 'o-', color=\"r\", label=\"Training score\")\n",
       "plt.plot(train_sizes, test_scores, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
       "\n",
       "plt.legend(loc=\"best\")\n",
       "plt.show()"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "### Example of Good Fit"
      ]
     },
     {
      "cell_type": "code",
      "metadata": {},
      "source": [
       "# Example plot for good fit (simulated)\n",
       "train_sizes = [50, 100, 150, 200, 250]\n",
       "train_scores = [0.85, 0.86, 0.87, 0.88, 0.89]\n",
       "test_scores = [0.83, 0.84, 0.85, 0.84, 0.85]\n",
       "\n",
       "plt.figure(figsize=(12, 8))\n",
       "plt.title(\"Example of Good Fit\")\n",
       "plt.xlabel(\"Training examples\")\n",
       "plt.ylabel(\"Score\")\n",
       "\n",
       "plt.plot(train_sizes, train_scores, 'o-', color=\"r\", label=\"Training score\")\n",
       "plt.plot(train_sizes, test_scores, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
       "\n",
       "plt.legend(loc=\"best\")\n",
       "plt.show()"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   